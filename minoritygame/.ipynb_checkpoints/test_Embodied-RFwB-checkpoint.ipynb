{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import importlib # debug \n",
    "#from misc_helpers import discount\n",
    "import IPython\n",
    "\n",
    "%autosave 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minority_agent\n",
    "from minority_env import MinorityGame1vN_env\n",
    "\n",
    "import embodiedMG as emg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python import debug as tf_debug # debug\n",
    "importlib.reload(minority_agent) \n",
    "importlib.reload(emg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minority Game Benchmark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 25\n",
    "n_epochs = 5001\n",
    "\n",
    "importlib.reload(minority_agent) \n",
    "importlib.reload(emg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "menv = MinorityGame1vN_env(33, 4, 4, 0.5)\n",
    "print(menv.state_space_size, menv.action_space_size)\n",
    "embrf = emg.EmbodiedAgentRFBaselined(name=\"mingame-RFB\", env_=menv)\n",
    "embrf.max_episode_length = 30 #101  # dangerous... may incentivize finite n behavior\n",
    "print(embrf, embrf.s_size, embrf.a_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "sess = tf.InteractiveSession()\n",
    "embrf.init_graph(sess) # note tboard log dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify step + play set up\n",
    "state = embrf.env.reset()\n",
    "print(state, embrf.act(state, sess))\n",
    "\n",
    "embrf.env.step(embrf.act(state, sess))\n",
    "embrf.play(sess)\n",
    "np.mean(np.array(embrf.episode_buffer)[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Baselining untrained pnet...')\n",
    "rwd_mg0 = []\n",
    "for k in range(num_episodes):\n",
    "    embrf.play(sess)\n",
    "    rwd_mg0.append(np.mean(np.array(embrf.episode_buffer)[:,2]))\n",
    "    if k%int(num_episodes/5) == 0: print(\"\\rEpisode {}/{}\".format(k, num_episodes),end=\"\")\n",
    "base_perf_mg = np.mean(rwd_mg0)\n",
    "print(\"\\nAgent wins an average of {} pct\".format(100.0*base_perf_mg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Agent w/ Algo on Experience Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train pnet on mingame episodes\n",
    "print('Training...')\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "embrf.work(sess, saver, num_epochs = n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pnet!\n",
    "print('Testing...')\n",
    "rwd_mg = []\n",
    "for k in range(num_episodes):\n",
    "    embrf.play(sess)\n",
    "    rwd_mg.append(np.mean(np.array(embrf.episode_buffer)[:,2]))\n",
    "    if k%int(num_episodes/5) == 0: print(\"\\rEpisode {}/{}\".format(k, num_episodes),end=\"\")\n",
    "trained_perf_mg = np.mean(rwd_mg)\n",
    "print(\"\\nAgent wins an average of {} pct compared to baseline of {} pct\".format(\n",
    "    100*trained_perf_mg, 100*base_perf_mg) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwd_mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "sns.boxplot(rwd_mg0, ax = axs[0])\n",
    "axs[0].set_title('Baseline Mean Success Percentage')\n",
    "sns.boxplot(rwd_mg, ax = axs[1])\n",
    "axs[1].set_title('Trained Mean Success Percentage')\n",
    "\n",
    "print(\"\\nAgent wins an average of {} pct \\ncompared to baseline of {} pct\".format(\n",
    "    100*np.mean(rwd_mg), 100*base_perf_mg) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embrf.play(sess)\n",
    "\n",
    "# len(embrf.episode_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tst = np.array(embrf.episode_buffer)\n",
    "# st = np.squeeze(tst[:,0])\n",
    "# at = np.squeeze(tst[:,1])\n",
    "# rt = np.squeeze(tst[:,2])\n",
    "# sprime_t = np.squeeze(tst[:,3])\n",
    "# (st, at, rt, sprime_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # sess.run(\n",
    "# # #     [embrf.value, embrf.lnPi_t, embrf.Advs_t, embrf.AdvlnPi_t, embrf.AdvVals_t, embrf.policy_LL],\n",
    "# # #     feed_dict={embrf.states_St: np.vstack(st),\n",
    "# # #             embrf.actions_At: np.vstack(at),\n",
    "# # #             embrf.returns_Gt: np.vstack(rt),\n",
    "# # #             embrf.states_St_prime: np.vstack(sprime_t)}\n",
    "# # # )\n",
    "\n",
    "# # # sess.run(\n",
    "# # #     [embrf.vloss, embrf.ploss, embrf.entropy],\n",
    "# # #     feed_dict={embrf.states_St: np.vstack(st),\n",
    "# # #             embrf.actions_At: np.vstack(at),\n",
    "# # #             embrf.returns_Gt: np.vstack(rt),\n",
    "# # #             embrf.states_St_prime: np.vstack(sprime_t)}\n",
    "# # # )\n",
    "\n",
    "# # [embrf.policy_vars, embrf.value_vars]\n",
    "\n",
    "# # sess.run(\n",
    "# #     embrf.p_grads,\n",
    "# #     feed_dict={embrf.states_St: np.vstack(st),\n",
    "# #             embrf.actions_At: np.vstack(at),\n",
    "# #             embrf.returns_Gt: np.vstack(rt),\n",
    "# #             embrf.states_St_prime: np.vstack(sprime_t)}\n",
    "# # )  # [embrf.val_grads, \n",
    "\n",
    "# sampgrads = sess.run(\n",
    "#     (embrf.optimizer_p.compute_gradients(loss=embrf.ploss, var_list=embrf.policy_vars)),\n",
    "#     feed_dict={embrf.states_St: np.vstack(st),\n",
    "#             embrf.actions_At: np.vstack(at),\n",
    "#             embrf.returns_Gt: np.vstack(rt),\n",
    "#             embrf.states_St_prime: np.vstack(sprime_t)}\n",
    "# ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
