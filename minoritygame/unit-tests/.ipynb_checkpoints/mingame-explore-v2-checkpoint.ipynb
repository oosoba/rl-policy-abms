{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment: Minority Game [1-vs-(N-1)]\n",
    "### Learner: Value-fxn-based + policy gradient\n",
    "#### O. Osoba\n",
    "#### Date: Jan-2019\n",
    "Exploring implementation of minority game with N players. <br> \n",
    "Issues: <br>\n",
    "- Appropriate decomposition of agent<->env<br>\n",
    "    - focusing on 1-vs.-(N-1) decomposition\n",
    "- Differences between std agent adaptation and REINFORCE adaptation? <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble: Libs + signal def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import glob, itertools, importlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML libs\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = './log/mingame'\n",
    "#tensorboard --logdir=mingame_worker_1:'./log/train_rf_mingame_worker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF on MinGame env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minoritygame.minority_agent as MGA\n",
    "import minoritygame.minority_env as MGE\n",
    "\n",
    "# Training Params\n",
    "lr = 1e-3\n",
    "gamma_reg = 0.001\n",
    "n_epochs = 500\n",
    "max_episode_length = 40\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "importlib.reload(MGE) \n",
    "importlib.reload(MGA)\n",
    "\n",
    "mingame = MGA.MinorityGameSingleAgent(#path=log_path,\n",
    "    nagents=33, m=3, s=4,\n",
    "    trainer=tf.train.AdamOptimizer(learning_rate=lr)\n",
    ")\n",
    "\n",
    "#sess.close()\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "n_epochs = 1000\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent rf_mingame_worker\n",
      "Step 0: (Policy LL, Entropy): ( -1221.926406, 0.692912 )\n",
      "Step, Average Rewards: (0, 99.0)\n",
      "Saved Model\n",
      "Step 50: (Policy LL, Entropy): ( -1300.679844, 0.682052 )\n",
      "Step, Average Rewards: (50, 93.56)\n",
      "Saved Model\n",
      "Step 100: (Policy LL, Entropy): ( -1080.306719, 0.679478 )\n",
      "Step, Average Rewards: (100, 92.24)\n",
      "Saved Model\n",
      "Step 150: (Policy LL, Entropy): ( -1305.036875, 0.690802 )\n",
      "Step, Average Rewards: (150, 94.76)\n",
      "Saved Model\n",
      "Step 200: (Policy LL, Entropy): ( -1088.030469, 0.693139 )\n",
      "Step, Average Rewards: (200, 92.3)\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "#work(self, sess,saver, num_epochs, gamma, max_episode_length=200):\n",
    "mingame.work(sess, saver, n_epochs, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1, r, _ = mingame.env.step(act_N=0)\n",
    "#print(s1, r) #print(mingame.act_rand(s1, sess)) #print(mingame.act(s1, sess))\n",
    "#plt.plot(mingame.episode_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
