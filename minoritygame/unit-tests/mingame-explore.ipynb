{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment: Minority Game\n",
    "### Learning: Value-fxn-free Policy Gradient (REINORCE)\n",
    "#### O. Osoba\n",
    "#### Date: Dec-2018\n",
    "Exploring implementation of minority game with N players. <br> \n",
    "Issues: <br>\n",
    "- Appropriate decomposition of agent<->env<br>\n",
    "    - focusing on 1-vs.-(N-1) decomposition\n",
    "- Differences between std agent adaptation and REINFORCE adaptation? <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble: Libs + signal def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import argparse, glob, itertools, importlib\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libs\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = './log/mingame'\n",
    "#tensorboard --logdir=mingame_worker_1:'./log/train_rf_mingame_worker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import minoritygame.minority as MG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = MG.MinorityGame(13, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.agents[0].strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.agents[0].vpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.agents[0].m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MG.repro_fig_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF on MinGame env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minoritygame.minority_agent as MGA\n",
    "import minoritygame.minority_env as MGE\n",
    "\n",
    "# Training Params\n",
    "lr = 1e-3\n",
    "gamma_reg = 0.001\n",
    "n_epochs = 500\n",
    "max_episode_length = 40\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent rf_mingame_worker\n",
      "Step 0: (Policy Loss, Value Loss): ( -3096.447500, 0.693123 )\n",
      "Step, Average Rewards: (0, 231.0)\n",
      "Saved Model\n",
      "Step 50: (Policy Loss, Value Loss): ( -2994.840250, 0.689118 )\n",
      "Step, Average Rewards: (50, 234.1)\n",
      "Saved Model\n",
      "Step 100: (Policy Loss, Value Loss): ( -2687.971000, 0.692830 )\n",
      "Step, Average Rewards: (100, 233.76)\n",
      "Saved Model\n",
      "Step 150: (Policy Loss, Value Loss): ( -3191.915250, 0.689895 )\n",
      "Step, Average Rewards: (150, 238.34)\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "importlib.reload(MGE) \n",
    "importlib.reload(MGA)\n",
    "\n",
    "mingame = MGA.MinorityGameSingleAgent(#path=log_path,\n",
    "    nagents=33, m=3, s=4,\n",
    "    trainer=tf.train.AdamOptimizer(learning_rate=lr)\n",
    ")\n",
    "\n",
    "#sess.close()\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "n_epochs = 1000\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "#work(self, sess, num_epochs, gamma, saver, max_episode_length=200):\n",
    "mingame.work(sess, n_epochs, gamma, saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1, r, _ = mingame.env.step(act_N=0)\n",
    "#print(s1, r) #print(mingame.act_rand(s1, sess)) #print(mingame.act(s1, sess))\n",
    "#plt.plot(mingame.episode_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
