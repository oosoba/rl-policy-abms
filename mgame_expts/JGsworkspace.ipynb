{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Resets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.13.1\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'embodied' from './embodied_arch/embodied.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, itertools, importlib, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# ML libs\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (18,7)\n",
    "\n",
    "log_path = './log/mingame'\n",
    "sys.path.append('./embodied_arch')\n",
    "import minoritygame.minority_env as MGE\n",
    "import tensorflow as tf\n",
    "import embodied as emg\n",
    "importlib.reload(MGE)\n",
    "importlib.reload(emg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "menv = MGE.MinorityGame1vN_env(nagents=301, m=2, s=2, mrl=3, p=0.5) # Creates the MGEnvironment\n",
    "embrf = emg.EmbodiedAgentRF(name=\"mgRF\", env_=menv, alpha=.000001) # Creates the learner, alpha scales losses, equivalent to LR?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard logs in:  ./log/train_mgRF\n",
      "[0 0 1] 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0]), 1.0, False, {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_episodes = 5 # Number of runs per update step?\n",
    "n_epochs = 200 #Number of times to update (If my memory serves me correctly)\n",
    "\n",
    "embrf.max_episode_length = 500 #This should be an environment parameter, I tihnk. \n",
    "sess = tf.InteractiveSession()\n",
    "embrf.init_graph(sess) # note tboard log dir\n",
    "\n",
    "## Verify step + play set up\n",
    "state = embrf.env.reset() \n",
    "print(state, embrf.act(state, sess)) \n",
    "\n",
    "embrf.env.step(embrf.act(state, sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baselining untrained pnet...\n",
      "Episode 4/5\n",
      "Agent wins an average of 50.879999999999995 pct\n"
     ]
    }
   ],
   "source": [
    "# ### Pre-test Agent\n",
    "print('Baselining untrained pnet...', flush=True)\n",
    "rwd_mg0 = []\n",
    "for k in range(num_episodes):\n",
    "    embrf.play(sess)\n",
    "    rwd_mg0.append(float(embrf.last_total_return)/embrf.max_episode_length)\n",
    "    #if k%int(num_episodes/5) == 0: \n",
    "    print(\"\\rEpisode {}/{}\".format(k, num_episodes),end=\"\")\n",
    "base_perf_mg = np.mean(rwd_mg0)\n",
    "print(\"\\nAgent wins an average of {} pct\".format(100.0*base_perf_mg), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Starting agent mgRF\n",
      "Epoch no.: 0/200\n",
      "Step 0: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [248.0, -0.13768339, 0.10811748] )\n",
      "Saved Model\n",
      "Epoch no.: 1/200\n",
      "Step 1: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [267.0, -0.16800764, 0.15076193] )\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved Model\n",
      "Epoch no.: 2/200\n",
      "Step 2: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [257.0, -0.20959897, 0.20545614] )\n",
      "Saved Model\n",
      "Epoch no.: 3/200\n",
      "Step 3: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [255.0, -0.28850323, 0.26401198] )\n",
      "Saved Model\n",
      "Epoch no.: 4/200\n",
      "Step 4: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [258.0, -0.36249083, 0.34490785] )\n",
      "Saved Model\n",
      "Epoch no.: 5/200\n",
      "Step 5: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [254.0, -0.45077544, 0.44904876] )\n",
      "Saved Model\n",
      "Epoch no.: 6/200\n",
      "Step 6: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [268.0, -0.55571204, 0.5545628] )\n",
      "Saved Model\n",
      "Epoch no.: 7/200\n",
      "Step 7: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [249.0, -0.6415594, 0.6426422] )\n",
      "Saved Model\n",
      "Epoch no.: 8/200\n",
      "Step 8: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [256.0, -0.6519262, 0.6674249] )\n",
      "Saved Model\n",
      "Epoch no.: 9/200\n",
      "Step 9: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [241.0, -0.6619132, 0.6524201] )\n",
      "Saved Model\n",
      "Epoch no.: 10/200\n",
      "Step 10: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [260.0, -0.5979766, 0.60100555] )\n",
      "Saved Model\n",
      "Epoch no.: 11/200\n",
      "Step 11: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [273.0, -0.53062844, 0.51617426] )\n",
      "Saved Model\n",
      "Epoch no.: 12/200\n",
      "Step 12: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [271.0, -0.43191743, 0.43328518] )\n",
      "Saved Model\n",
      "Epoch no.: 13/200\n",
      "Step 13: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [274.0, -0.35033137, 0.34038022] )\n",
      "Saved Model\n",
      "Epoch no.: 14/200\n",
      "Step 14: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [271.0, -0.29615533, 0.28436267] )\n",
      "Saved Model\n",
      "Epoch no.: 15/200\n",
      "Step 15: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [286.0, -0.27645433, 0.25470188] )\n",
      "Saved Model\n",
      "Epoch no.: 16/200\n",
      "Step 16: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [296.0, -0.27423143, 0.25268438] )\n",
      "Saved Model\n",
      "Epoch no.: 17/200\n",
      "Step 17: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [292.0, -0.28650457, 0.27379608] )\n",
      "Saved Model\n",
      "Epoch no.: 18/200\n",
      "Step 18: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [281.0, -0.33259833, 0.31437755] )\n",
      "Saved Model\n",
      "Epoch no.: 19/200\n",
      "Step 19: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [307.0, -0.29227164, 0.347396] )\n",
      "Saved Model\n",
      "Epoch no.: 20/200\n",
      "Step 20: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [307.0, -0.3142588, 0.33103073] )\n",
      "Saved Model\n",
      "Epoch no.: 21/200\n",
      "Step 21: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [294.0, -0.33279493, 0.31348646] )\n",
      "Saved Model\n",
      "Epoch no.: 22/200\n",
      "Step 22: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [304.0, -0.24156883, 0.2922909] )\n",
      "Saved Model\n",
      "Epoch no.: 23/200\n",
      "Step 23: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [292.0, -0.25134188, 0.2502105] )\n",
      "Saved Model\n",
      "Epoch no.: 24/200\n",
      "Step 24: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [293.0, -0.2365059, 0.20974837] )\n",
      "Saved Model\n",
      "Epoch no.: 25/200\n",
      "Step 25: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [301.0, -0.18633145, 0.18368798] )\n",
      "Saved Model\n",
      "Epoch no.: 26/200\n",
      "Step 26: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [294.0, -0.14947286, 0.16628304] )\n",
      "Saved Model\n",
      "Epoch no.: 27/200\n",
      "Step 27: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [284.0, -0.12971057, 0.15220836] )\n",
      "Saved Model\n",
      "Epoch no.: 28/200\n",
      "Step 28: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [283.0, -0.13050207, 0.13560474] )\n",
      "Saved Model\n",
      "Epoch no.: 29/200\n",
      "Step 29: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [286.0, -0.12069417, 0.12287188] )\n",
      "Saved Model\n",
      "Epoch no.: 30/200\n",
      "Step 30: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [285.0, -0.15456942, 0.114143185] )\n",
      "Saved Model\n",
      "Epoch no.: 31/200\n",
      "Step 31: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [282.0, -0.12355709, 0.11177301] )\n",
      "Saved Model\n",
      "Epoch no.: 32/200\n",
      "Step 32: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [278.0, -0.10492403, 0.111886315] )\n",
      "Saved Model\n",
      "Epoch no.: 33/200\n",
      "Step 33: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [275.0, -0.1098496, 0.110911645] )\n",
      "Saved Model\n",
      "Epoch no.: 34/200\n",
      "Step 34: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [274.0, -0.10734958, 0.10866088] )\n",
      "Saved Model\n",
      "Epoch no.: 35/200\n",
      "Step 35: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [274.0, -0.10916804, 0.10529464] )\n",
      "Saved Model\n",
      "Epoch no.: 36/200\n",
      "Step 36: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [273.0, -0.103130996, 0.10067112] )\n",
      "Saved Model\n",
      "Epoch no.: 37/200\n",
      "Step 37: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [271.0, -0.10961242, 0.095102414] )\n",
      "Saved Model\n",
      "Epoch no.: 38/200\n",
      "Step 38: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [259.0, -0.06197164, 0.091556005] )\n",
      "Saved Model\n",
      "Epoch no.: 39/200\n",
      "Step 39: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [268.0, -0.095532775, 0.079553224] )\n",
      "Saved Model\n",
      "Epoch no.: 40/200\n",
      "Step 40: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [262.0, -0.062931195, 0.07356375] )\n",
      "Saved Model\n",
      "Epoch no.: 41/200\n",
      "Step 41: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [268.0, -0.08173762, 0.067816146] )\n",
      "Saved Model\n",
      "Epoch no.: 42/200\n",
      "Step 42: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [264.0, -0.074901655, 0.069304354] )\n",
      "Saved Model\n",
      "Epoch no.: 43/200\n",
      "Step 43: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [260.0, -0.063366294, 0.072540596] )\n",
      "Saved Model\n",
      "Epoch no.: 44/200\n",
      "Step 44: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [262.0, -0.06912961, 0.07398221] )\n",
      "Saved Model\n",
      "Epoch no.: 45/200\n",
      "Step 45: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [268.0, -0.08508252, 0.075154096] )\n",
      "Saved Model\n",
      "Epoch no.: 46/200\n",
      "Step 46: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [263.0, -0.083281405, 0.08027012] )\n",
      "Saved Model\n",
      "Epoch no.: 47/200\n",
      "Step 47: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [272.0, -0.092083655, 0.0842278] )\n",
      "Saved Model\n",
      "Epoch no.: 48/200\n",
      "Step 48: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [271.0, -0.09823608, 0.09194753] )\n",
      "Saved Model\n",
      "Epoch no.: 49/200\n",
      "Step 49: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [264.0, -0.09140432, 0.100146726] )\n",
      "Saved Model\n",
      "Epoch no.: 50/200\n",
      "Step 50: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [286.0, -0.11660742, 0.10186018] )\n",
      "Saved Model\n",
      "Epoch no.: 51/200\n",
      "Step 51: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [284.0, -0.10517901, 0.10754674] )\n",
      "Saved Model\n",
      "Epoch no.: 52/200\n",
      "Step 52: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [289.0, -0.12232565, 0.11156431] )\n",
      "Saved Model\n",
      "Epoch no.: 53/200\n",
      "Step 53: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [297.0, -0.12951414, 0.11463782] )\n",
      "Saved Model\n",
      "Epoch no.: 54/200\n",
      "Step 54: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [298.0, -0.14702891, 0.119823664] )\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no.: 55/200\n",
      "Step 55: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [293.0, -0.17090298, 0.13623525] )\n",
      "Saved Model\n",
      "Epoch no.: 56/200\n",
      "Step 56: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [314.0, -0.18846661, 0.16519874] )\n",
      "Saved Model\n",
      "Epoch no.: 57/200\n",
      "Step 57: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [306.0, -0.21597584, 0.21696237] )\n",
      "Saved Model\n",
      "Epoch no.: 58/200\n",
      "Step 58: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [331.0, -0.26951623, 0.25989085] )\n",
      "Saved Model\n",
      "Epoch no.: 59/200\n",
      "Step 59: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [338.0, -0.2687553, 0.323227] )\n",
      "Saved Model\n",
      "Epoch no.: 60/200\n",
      "Step 60: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [345.0, -0.37466055, 0.34336042] )\n",
      "Saved Model\n",
      "Epoch no.: 61/200\n",
      "Step 61: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [332.0, -0.38673162, 0.39267537] )\n",
      "Saved Model\n",
      "Epoch no.: 62/200\n",
      "Step 62: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [345.0, -0.38726375, 0.40486088] )\n",
      "Saved Model\n",
      "Epoch no.: 63/200\n",
      "Step 63: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [363.0, -0.34331256, 0.39975533] )\n",
      "Saved Model\n",
      "Epoch no.: 64/200\n",
      "Step 64: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [347.0, -0.39113608, 0.33528665] )\n",
      "Saved Model\n",
      "Epoch no.: 65/200\n",
      "Step 65: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [357.0, -0.26138118, 0.2973342] )\n",
      "Saved Model\n",
      "Epoch no.: 66/200\n",
      "Step 66: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [351.0, -0.2309133, 0.23404282] )\n",
      "Saved Model\n",
      "Epoch no.: 67/200\n",
      "Step 67: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [360.0, -0.17999533, 0.18940485] )\n",
      "Saved Model\n",
      "Epoch no.: 68/200\n",
      "Step 68: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [351.0, -0.20576964, 0.16653776] )\n",
      "Saved Model\n",
      "Epoch no.: 69/200\n",
      "Step 69: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [350.0, -0.14125124, 0.15356685] )\n",
      "Saved Model\n",
      "Epoch no.: 70/200\n",
      "Step 70: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [351.0, -0.14164469, 0.14227545] )\n",
      "Saved Model\n",
      "Epoch no.: 71/200\n",
      "Step 71: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [342.0, -0.18497795, 0.13552038] )\n",
      "Saved Model\n",
      "Epoch no.: 72/200\n",
      "Step 72: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [357.0, -0.13121301, 0.13526148] )\n",
      "Saved Model\n",
      "Epoch no.: 73/200\n",
      "Step 73: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [355.0, -0.114400454, 0.13160658] )\n",
      "Saved Model\n",
      "Epoch no.: 74/200\n",
      "Step 74: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [350.0, -0.1187344, 0.12387721] )\n",
      "Saved Model\n",
      "Epoch no.: 75/200\n",
      "Step 75: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [353.0, -0.10942936, 0.12045333] )\n",
      "Saved Model\n",
      "Epoch no.: 76/200\n",
      "Step 76: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [354.0, -0.13611236, 0.11815579] )\n",
      "Saved Model\n",
      "Epoch no.: 77/200\n",
      "Step 77: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [352.0, -0.11984238, 0.11485756] )\n",
      "Saved Model\n",
      "Epoch no.: 78/200\n",
      "Step 78: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [359.0, -0.09354737, 0.11315923] )\n",
      "Saved Model\n",
      "Epoch no.: 79/200\n",
      "Step 79: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [351.0, -0.1218053, 0.10775983] )\n",
      "Saved Model\n",
      "Epoch no.: 80/200\n",
      "Step 80: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [355.0, -0.10724819, 0.107317135] )\n",
      "Saved Model\n",
      "Epoch no.: 81/200\n",
      "Step 81: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [354.0, -0.11077505, 0.10946314] )\n",
      "Saved Model\n",
      "Epoch no.: 82/200\n",
      "Step 82: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [353.0, -0.12927383, 0.11448777] )\n",
      "Saved Model\n",
      "Epoch no.: 83/200\n",
      "Step 83: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [353.0, -0.115039095, 0.121571735] )\n",
      "Saved Model\n",
      "Epoch no.: 84/200\n",
      "Step 84: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [354.0, -0.110547855, 0.12766376] )\n",
      "Saved Model\n",
      "Epoch no.: 85/200\n",
      "Step 85: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [341.0, -0.14799644, 0.12746681] )\n",
      "Saved Model\n",
      "Epoch no.: 86/200\n",
      "Step 86: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [352.0, -0.13096043, 0.13642578] )\n",
      "Saved Model\n",
      "Epoch no.: 87/200\n",
      "Step 87: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [342.0, -0.15501356, 0.13522695] )\n",
      "Saved Model\n",
      "Epoch no.: 88/200\n",
      "Step 88: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [336.0, -0.15714347, 0.13655156] )\n",
      "Saved Model\n",
      "Epoch no.: 89/200\n",
      "Step 89: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [342.0, -0.14244077, 0.14553119] )\n",
      "Saved Model\n",
      "Epoch no.: 90/200\n",
      "Step 90: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [344.0, -0.13488811, 0.14887725] )\n",
      "Saved Model\n",
      "Epoch no.: 91/200\n",
      "Step 91: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [348.0, -0.12725224, 0.14558773] )\n",
      "Saved Model\n",
      "Epoch no.: 92/200\n",
      "Step 92: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [346.0, -0.13306329, 0.13265546] )\n",
      "Saved Model\n",
      "Epoch no.: 93/200\n",
      "Step 93: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [343.0, -0.14213131, 0.11960422] )\n",
      "Saved Model\n",
      "Epoch no.: 94/200\n",
      "Step 94: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [351.0, -0.116866805, 0.116788186] )\n",
      "Saved Model\n",
      "Epoch no.: 95/200\n",
      "Step 95: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [355.0, -0.104571074, 0.11710796] )\n",
      "Saved Model\n",
      "Epoch no.: 96/200\n",
      "Step 96: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [345.0, -0.14015083, 0.11397296] )\n",
      "Saved Model\n",
      "Epoch no.: 97/200\n",
      "Step 97: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [349.0, -0.13450569, 0.117063984] )\n",
      "Saved Model\n",
      "Epoch no.: 98/200\n",
      "Step 98: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [345.0, -0.14491561, 0.12029195] )\n",
      "Saved Model\n",
      "Epoch no.: 99/200\n",
      "Step 99: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [346.0, -0.14676851, 0.1266767] )\n",
      "Saved Model\n",
      "Epoch no.: 100/200\n",
      "Step 100: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [345.0, -0.12510446, 0.13032767] )\n",
      "Saved Model\n",
      "Epoch no.: 101/200\n",
      "Step 101: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [341.0, -0.1506597, 0.13427241] )\n",
      "Saved Model\n",
      "Epoch no.: 102/200\n",
      "Step 102: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [351.0, -0.13348018, 0.13854499] )\n",
      "Saved Model\n",
      "Epoch no.: 103/200\n",
      "Step 103: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [349.0, -0.1313833, 0.13173541] )\n",
      "Saved Model\n",
      "Epoch no.: 104/200\n",
      "Step 104: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [350.0, -0.11739531, 0.119662456] )\n",
      "Saved Model\n",
      "Epoch no.: 105/200\n",
      "Step 105: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [354.0, -0.1129281, 0.10900368] )\n",
      "Saved Model\n",
      "Epoch no.: 106/200\n",
      "Step 106: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [360.0, -0.086085744, 0.09802153] )\n",
      "Saved Model\n",
      "Epoch no.: 107/200\n",
      "Step 107: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [361.0, -0.08293123, 0.0877515] )\n",
      "Saved Model\n",
      "Epoch no.: 108/200\n",
      "Step 108: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [366.0, -0.062224265, 0.07987932] )\n",
      "Saved Model\n",
      "Epoch no.: 109/200\n",
      "Step 109: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [369.0, -0.049475964, 0.07157362] )\n",
      "Saved Model\n",
      "Epoch no.: 110/200\n",
      "Step 110: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [360.0, -0.10463736, 0.06367871] )\n",
      "Saved Model\n",
      "Epoch no.: 111/200\n",
      "Step 111: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [371.0, -0.031977687, 0.06054607] )\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no.: 112/200\n",
      "Step 112: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [369.0, -0.044290945, 0.056864996] )\n",
      "Saved Model\n",
      "Epoch no.: 113/200\n",
      "Step 113: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [369.0, -0.044420283, 0.053131875] )\n",
      "Saved Model\n",
      "Epoch no.: 114/200\n",
      "Step 114: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.023845326, 0.04951277] )\n",
      "Saved Model\n",
      "Epoch no.: 115/200\n",
      "Step 115: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.019732486, 0.045238923] )\n",
      "Saved Model\n",
      "Epoch no.: 116/200\n",
      "Step 116: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.027456777, 0.04050873] )\n",
      "Saved Model\n",
      "Epoch no.: 117/200\n",
      "Step 117: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.022211308, 0.03606484] )\n",
      "Saved Model\n",
      "Epoch no.: 118/200\n",
      "Step 118: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [370.0, -0.04255064, 0.031936973] )\n",
      "Saved Model\n",
      "Epoch no.: 119/200\n",
      "Step 119: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [371.0, -0.030284872, 0.028973993] )\n",
      "Saved Model\n",
      "Epoch no.: 120/200\n",
      "Step 120: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.013098979, 0.026508622] )\n",
      "Saved Model\n",
      "Epoch no.: 121/200\n",
      "Step 121: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.021814415, 0.024152273] )\n",
      "Saved Model\n",
      "Epoch no.: 122/200\n",
      "Step 122: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.012713242, 0.02211754] )\n",
      "Saved Model\n",
      "Epoch no.: 123/200\n",
      "Step 123: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.012490752, 0.020217828] )\n",
      "Saved Model\n",
      "Epoch no.: 124/200\n",
      "Step 124: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.012289952, 0.018511526] )\n",
      "Saved Model\n",
      "Epoch no.: 125/200\n",
      "Step 125: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0027930413, 0.017079432] )\n",
      "Saved Model\n",
      "Epoch no.: 126/200\n",
      "Step 126: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [371.0, -0.03647491, 0.015652899] )\n",
      "Saved Model\n",
      "Epoch no.: 127/200\n",
      "Step 127: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.012181391, 0.014674361] )\n",
      "Saved Model\n",
      "Epoch no.: 128/200\n",
      "Step 128: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.021345168, 0.0138695035] )\n",
      "Saved Model\n",
      "Epoch no.: 129/200\n",
      "Step 129: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0021524075, 0.013453892] )\n",
      "Saved Model\n",
      "Epoch no.: 130/200\n",
      "Step 130: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [371.0, -0.0313018, 0.01290872] )\n",
      "Saved Model\n",
      "Epoch no.: 131/200\n",
      "Step 131: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0020736947, 0.012843814] )\n",
      "Saved Model\n",
      "Epoch no.: 132/200\n",
      "Step 132: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011417414, 0.012577281] )\n",
      "Saved Model\n",
      "Epoch no.: 133/200\n",
      "Step 133: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0020211008, 0.012411186] )\n",
      "Saved Model\n",
      "Epoch no.: 134/200\n",
      "Step 134: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.02184041, 0.01198204] )\n",
      "Saved Model\n",
      "Epoch no.: 135/200\n",
      "Step 135: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.020528134, 0.011950676] )\n",
      "Saved Model\n",
      "Epoch no.: 136/200\n",
      "Step 136: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011157049, 0.012354402] )\n",
      "Saved Model\n",
      "Epoch no.: 137/200\n",
      "Step 137: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.040063813, 0.012815705] )\n",
      "Saved Model\n",
      "Epoch no.: 138/200\n",
      "Step 138: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [371.0, -0.030283941, 0.01335924] )\n",
      "Saved Model\n",
      "Epoch no.: 139/200\n",
      "Step 139: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.019376252, 0.014988632] )\n",
      "Saved Model\n",
      "Epoch no.: 140/200\n",
      "Step 140: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.013250611, 0.016714524] )\n",
      "Saved Model\n",
      "Epoch no.: 141/200\n",
      "Step 141: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011207982, 0.0179789] )\n",
      "Saved Model\n",
      "Epoch no.: 142/200\n",
      "Step 142: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [370.0, -0.036433995, 0.018699016] )\n",
      "Saved Model\n",
      "Epoch no.: 143/200\n",
      "Step 143: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [371.0, -0.030752407, 0.021272162] )\n",
      "Saved Model\n",
      "Epoch no.: 144/200\n",
      "Step 144: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.01898784, 0.024670886] )\n",
      "Saved Model\n",
      "Epoch no.: 145/200\n",
      "Step 145: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.019202575, 0.027562063] )\n",
      "Saved Model\n",
      "Epoch no.: 146/200\n",
      "Step 146: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.015098042, 0.028603334] )\n",
      "Saved Model\n",
      "Epoch no.: 147/200\n",
      "Step 147: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.02115163, 0.025460104] )\n",
      "Saved Model\n",
      "Epoch no.: 148/200\n",
      "Step 148: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011933447, 0.02296286] )\n",
      "Saved Model\n",
      "Epoch no.: 149/200\n",
      "Step 149: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [371.0, -0.026869783, 0.019906752] )\n",
      "Saved Model\n",
      "Epoch no.: 150/200\n",
      "Step 150: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011367828, 0.018106177] )\n",
      "Saved Model\n",
      "Epoch no.: 151/200\n",
      "Step 151: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.012751133, 0.016022716] )\n",
      "Saved Model\n",
      "Epoch no.: 152/200\n",
      "Step 152: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0025027592, 0.014084199] )\n",
      "Saved Model\n",
      "Epoch no.: 153/200\n",
      "Step 153: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011130113, 0.012013046] )\n",
      "Saved Model\n",
      "Epoch no.: 154/200\n",
      "Step 154: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.020663245, 0.010501607] )\n",
      "Saved Model\n",
      "Epoch no.: 155/200\n",
      "Step 155: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0015786015, 0.009786073] )\n",
      "Saved Model\n",
      "Epoch no.: 156/200\n",
      "Step 156: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0014167029, 0.008977656] )\n",
      "Saved Model\n",
      "Epoch no.: 157/200\n",
      "Step 157: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.021554407, 0.008120281] )\n",
      "Saved Model\n",
      "Epoch no.: 158/200\n",
      "Step 158: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011428984, 0.0077850455] )\n",
      "Saved Model\n",
      "Epoch no.: 159/200\n",
      "Step 159: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0011536372, 0.0076094884] )\n",
      "Saved Model\n",
      "Epoch no.: 160/200\n",
      "Step 160: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0011023822, 0.0073326654] )\n",
      "Saved Model\n",
      "Epoch no.: 161/200\n",
      "Step 161: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.02196982, 0.0069778417] )\n",
      "Saved Model\n",
      "Epoch no.: 162/200\n",
      "Step 162: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0010461878, 0.0070163547] )\n",
      "Saved Model\n",
      "Epoch no.: 163/200\n",
      "Step 163: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0010339639, 0.0069403793] )\n",
      "Saved Model\n",
      "Epoch no.: 164/200\n",
      "Step 164: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011458819, 0.006775762] )\n",
      "Saved Model\n",
      "Epoch no.: 165/200\n",
      "Step 165: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0010042434, 0.0067633633] )\n",
      "Saved Model\n",
      "Epoch no.: 166/200\n",
      "Step 166: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.0114441, 0.006647228] )\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no.: 167/200\n",
      "Step 167: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.01141322, 0.006672895] )\n",
      "Saved Model\n",
      "Epoch no.: 168/200\n",
      "Step 168: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0010235856, 0.0068325396] )\n",
      "Saved Model\n",
      "Epoch no.: 169/200\n",
      "Step 169: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011316837, 0.0068663643] )\n",
      "Saved Model\n",
      "Epoch no.: 170/200\n",
      "Step 170: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0010662241, 0.007030309] )\n",
      "Saved Model\n",
      "Epoch no.: 171/200\n",
      "Step 171: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0010737772, 0.007056859] )\n",
      "Saved Model\n",
      "Epoch no.: 172/200\n",
      "Step 172: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0010577745, 0.0069631077] )\n",
      "Saved Model\n",
      "Epoch no.: 173/200\n",
      "Step 173: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0010231857, 0.0067750867] )\n",
      "Saved Model\n",
      "Epoch no.: 174/200\n",
      "Step 174: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0009746449, 0.0065144566] )\n",
      "Saved Model\n",
      "Epoch no.: 175/200\n",
      "Step 175: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011397931, 0.0061983285] )\n",
      "Saved Model\n",
      "Epoch no.: 176/200\n",
      "Step 176: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.00088914484, 0.006046268] )\n",
      "Saved Model\n",
      "Epoch no.: 177/200\n",
      "Step 177: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.00085139263, 0.0058357622] )\n",
      "Saved Model\n",
      "Epoch no.: 178/200\n",
      "Step 178: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011554442, 0.005582043] )\n",
      "Saved Model\n",
      "Epoch no.: 179/200\n",
      "Step 179: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0007858962, 0.005463953] )\n",
      "Saved Model\n",
      "Epoch no.: 180/200\n",
      "Step 180: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0007571089, 0.0052976334] )\n",
      "Saved Model\n",
      "Epoch no.: 181/200\n",
      "Step 181: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011690156, 0.0050945147] )\n",
      "Saved Model\n",
      "Epoch no.: 182/200\n",
      "Step 182: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.022674548, 0.005030837] )\n",
      "Saved Model\n",
      "Epoch no.: 183/200\n",
      "Step 183: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.00075909187, 0.005272714] )\n",
      "Saved Model\n",
      "Epoch no.: 184/200\n",
      "Step 184: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.02788021, 0.005444642] )\n",
      "Saved Model\n",
      "Epoch no.: 185/200\n",
      "Step 185: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.011347411, 0.0057910797] )\n",
      "Saved Model\n",
      "Epoch no.: 186/200\n",
      "Step 186: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0009508454, 0.0062719397] )\n",
      "Saved Model\n",
      "Epoch no.: 187/200\n",
      "Step 187: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.014044313, 0.0066404347] )\n",
      "Saved Model\n",
      "Epoch no.: 188/200\n",
      "Step 188: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0011072865, 0.0070770737] )\n",
      "Saved Model\n",
      "Epoch no.: 189/200\n",
      "Step 189: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.023679715, 0.0073463805] )\n",
      "Saved Model\n",
      "Epoch no.: 190/200\n",
      "Step 190: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0012811554, 0.007961834] )\n",
      "Saved Model\n",
      "Epoch no.: 191/200\n",
      "Step 191: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.010886682, 0.008358642] )\n",
      "Saved Model\n",
      "Epoch no.: 192/200\n",
      "Step 192: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.001464456, 0.008874173] )\n",
      "Saved Model\n",
      "Epoch no.: 193/200\n",
      "Step 193: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.010835545, 0.009097661] )\n",
      "Saved Model\n",
      "Epoch no.: 194/200\n",
      "Step 194: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0015576096, 0.00933438] )\n",
      "Saved Model\n",
      "Epoch no.: 195/200\n",
      "Step 195: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0015427037, 0.009268452] )\n",
      "Saved Model\n",
      "Epoch no.: 196/200\n",
      "Step 196: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.020272737, 0.008979775] )\n",
      "Saved Model\n",
      "Epoch no.: 197/200\n",
      "Step 197: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [374.0, -0.0015188293, 0.009156232] )\n",
      "Saved Model\n",
      "Epoch no.: 198/200\n",
      "Step 198: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [372.0, -0.020229358, 0.0090874545] )\n",
      "Saved Model\n",
      "Epoch no.: 199/200\n",
      "Step 199: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [373.0, -0.010826667, 0.00947253] )\n",
      "Saved Model\n",
      "WARNING:tensorflow:From <ipython-input-5-2c013e02a2bd>:8: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "Training...\n",
      "Starting agent mgRF\n",
      "Epoch no.: 0/200\n",
      "Step 0: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [268.0, -0.21359406, 0.23185703] )\n",
      "Saved Model\n",
      "Epoch no.: 1/200\n",
      "Step 1: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [262.0, -0.151378, 0.18981367] )\n",
      "Saved Model\n",
      "Epoch no.: 2/200\n",
      "Step 2: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [259.0, -0.08635648, 0.11293763] )\n",
      "Saved Model\n",
      "Epoch no.: 3/200\n",
      "Step 3: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [256.0, -0.07150369, 0.06922122] )\n",
      "Saved Model\n",
      "Epoch no.: 4/200\n",
      "Step 4: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [254.0, -0.065274075, 0.048920974] )\n",
      "Saved Model\n",
      "Epoch no.: 5/200\n",
      "Step 5: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [254.0, -0.036386847, 0.038794376] )\n",
      "Saved Model\n",
      "Epoch no.: 6/200\n",
      "Step 6: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [252.0, -0.019416569, 0.032448206] )\n",
      "Saved Model\n",
      "Epoch no.: 7/200\n",
      "Step 7: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [253.0, -0.03020314, 0.026862947] )\n",
      "Saved Model\n",
      "Epoch no.: 8/200\n",
      "Step 8: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [251.0, -0.011606639, 0.02337224] )\n",
      "Saved Model\n",
      "Epoch no.: 9/200\n",
      "Step 9: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.0038059892, 0.019822057] )\n",
      "Saved Model\n",
      "Epoch no.: 10/200\n",
      "Step 10: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [252.0, -0.02366938, 0.015952256] )\n",
      "Saved Model\n",
      "Epoch no.: 11/200\n",
      "Step 11: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [251.0, -0.010645874, 0.013470317] )\n",
      "Saved Model\n",
      "Epoch no.: 12/200\n",
      "Step 12: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.0019849823, 0.011550629] )\n",
      "Saved Model\n",
      "Epoch no.: 13/200\n",
      "Step 13: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [251.0, -0.0106493225, 0.0097018685] )\n",
      "Saved Model\n",
      "Epoch no.: 14/200\n",
      "Step 14: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [251.0, -0.010731549, 0.008393046] )\n",
      "Saved Model\n",
      "Epoch no.: 15/200\n",
      "Step 15: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [251.0, -0.0108144665, 0.007503854] )\n",
      "Saved Model\n",
      "Epoch no.: 16/200\n",
      "Step 16: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [251.0, -0.0108781615, 0.006924183] )\n",
      "Saved Model\n",
      "Epoch no.: 17/200\n",
      "Step 17: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.0010066556, 0.0064615137] )\n",
      "Saved Model\n",
      "Epoch no.: 18/200\n",
      "Step 18: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.0009156509, 0.0059486995] )\n",
      "Saved Model\n",
      "Epoch no.: 19/200\n",
      "Step 19: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.00082567777, 0.005437679] )\n",
      "Saved Model\n",
      "Epoch no.: 20/200\n",
      "Step 20: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.000736135, 0.004925056] )\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no.: 21/200\n",
      "Step 21: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.0006507085, 0.004429336] )\n",
      "Saved Model\n",
      "Epoch no.: 22/200\n",
      "Step 22: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.0005719005, 0.0039643757] )\n",
      "Saved Model\n",
      "Epoch no.: 23/200\n",
      "Step 23: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.0005015116, 0.0035410356] )\n",
      "Saved Model\n",
      "Epoch no.: 24/200\n",
      "Step 24: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.0004391898, 0.0031590308] )\n",
      "Saved Model\n",
      "Epoch no.: 25/200\n",
      "Step 25: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.0003844955, 0.0028168086] )\n",
      "Saved Model\n",
      "Epoch no.: 26/200\n",
      "Step 26: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.00033719733, 0.002515125] )\n",
      "Saved Model\n",
      "Epoch no.: 27/200\n",
      "Step 27: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.0002963925, 0.0022495913] )\n",
      "Saved Model\n",
      "Epoch no.: 28/200\n",
      "Step 28: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.00026141567, 0.0020174356] )\n",
      "Saved Model\n",
      "Epoch no.: 29/200\n",
      "Step 29: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.00023139606, 0.0018145747] )\n",
      "Saved Model\n",
      "Epoch no.: 30/200\n",
      "Step 30: Stats(['Perf/Recent Reward', 'Losses/Policy LL', 'Losses/Entropy']): ( [250.0, -0.00020559764, 0.00163717] )\n",
      "Saved Model\n",
      "Epoch no.: 31/200"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2c013e02a2bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtotals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mseqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/aiabm/multiagentABM/embodied_arch/embodied.py\u001b[0m in \u001b[0;36mwork\u001b[0;34m(self, sess, saver, num_epochs, gamma)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0;31m# TRAIN: Update the network using episodes in the buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/aiabm/multiagentABM/embodied_arch/embodied.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, sess)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# generate a full episode rollout (self.brain.episode_buffer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episode_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mact_pn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# print(\"Selected Action: \", act_pn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_pn\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get next state, reward, & done flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_pn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/aiabm/multiagentABM/embodied_arch/embodied.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, sess)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m# return np.array(a_t).squeeze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapnprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates_St\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0ma_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1137\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \"\"\"\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 300\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    301\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3480\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3496\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_tensor and allow_operation can't both be False.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3498\u001b[0;31m     \u001b[0mtemp_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtemp_obj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3500\u001b[0m       \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \"\"\"\n\u001b[0;32m--> 146\u001b[0;31m   \u001b[0mconv_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_as_graph_element\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconv_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconv_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seqs = []\n",
    "for i in range(2):\n",
    "    print('Training...')\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    totals = embrf.work(sess, saver, num_epochs = n_epochs)\n",
    "    seqs.append(totals)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    adam_vars = [var for var in tf.all_variables() if 'adam' in var.name]\n",
    "    sess.run(tf.variables_initializer(adam_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Post-test Agent# Test pnet!\n",
    "print('Testing...', flush=True)\n",
    "rwd_mg = []\n",
    "for k in range(num_episodes):\n",
    "    embrf.play(sess)\n",
    "    rwd_mg.append(float(embrf.last_total_return)/embrf.max_episode_length)\n",
    "    if k%int(num_episodes/5) == 0: \n",
    "        print(\"\\rEpisode {}/{}\".format(k, num_episodes),end=\"\")\n",
    "trained_perf_mg = np.mean(rwd_mg)\n",
    "print(\"\\nAgent wins an average of {} pct \\ncompared to baseline of {} pct\".format(\n",
    "    100*trained_perf_mg, 100*base_perf_mg), flush=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.trainable_variables()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.trainable_variables()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embrf.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
