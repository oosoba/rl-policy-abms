{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "import IPython\n",
    "\n",
    "%autosave 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./..')\n",
    "\n",
    "import minority_agent\n",
    "from minority_env import MinorityGame1vN_env\n",
    "\n",
    "import embodiedMG as emg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(minority_agent) \n",
    "importlib.reload(emg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minority Game Benchmark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "n_epochs = 50001\n",
    "\n",
    "importlib.reload(minority_agent) \n",
    "importlib.reload(emg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "menv = MinorityGame1vN_env(33, 4, 4, 0.5)\n",
    "embrf = emg.EmbodiedAgentRFBaselined(\n",
    "    name=\"mingame-RFB\", \n",
    "    env_=menv,\n",
    "    alpha_p=5.e-2, alpha_v=1.e-1\n",
    ")\n",
    "\n",
    "print(menv.state_space_size, menv.action_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embrf.max_episode_length = 30 #101  # dangerous... may incentivize finite n behavior\n",
    "print(embrf, embrf.s_size, embrf.a_size)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "embrf.init_graph(sess) # note tboard log dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify step + play set up\n",
    "state = embrf.env.reset()\n",
    "print(state, embrf.act(state, sess))\n",
    "\n",
    "embrf.env.step(embrf.act(state, sess))\n",
    "embrf.play(sess)\n",
    "embrf.last_total_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Baselining untrained pnet...')\n",
    "rwd_mg0 = []\n",
    "for k in range(num_episodes):\n",
    "    embrf.play(sess)\n",
    "    rwd_mg0.append(embrf.last_total_return)\n",
    "    if k%int(num_episodes/5) == 0: print(\"\\rEpisode {}/{}\".format(k, num_episodes),end=\"\")\n",
    "base_perf_mg = np.mean(rwd_mg0)\n",
    "print(\"\\nAgent wins an average of {} pct\".format(100.0*base_perf_mg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Agent w/ Algo on Experience Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train pnet on mingame episodes\n",
    "print('Training...')\n",
    "n_epochs = 501\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "embrf.work(sess, saver, num_epochs = n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pnet!\n",
    "print('Testing...')\n",
    "rwd_mg = []\n",
    "for k in range(num_episodes):\n",
    "    embrf.play(sess)\n",
    "    rwd_mg.append(embrf.last_total_return)\n",
    "    if k%int(num_episodes/5) == 0: print(\"\\rEpisode {}/{}\".format(k, num_episodes),end=\"\")\n",
    "trained_perf_mg = np.mean(rwd_mg)\n",
    "print(\"\\nAgent wins an average of {} pct compared to baseline of {} pct\".format(\n",
    "    100*trained_perf_mg, 100*base_perf_mg) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwd_mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "sns.boxplot(rwd_mg0, ax = axs[0])\n",
    "axs[0].set_title('Baseline Mean Success Percentage')\n",
    "sns.boxplot(rwd_mg, ax = axs[1])\n",
    "axs[1].set_title('Trained Mean Success Percentage')\n",
    "\n",
    "print(\"\\nAgent wins an average of {} pct \\ncompared to baseline of {} pct\".format(\n",
    "    100*np.mean(rwd_mg), 100*base_perf_mg) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embrf.play(sess)\n",
    "\n",
    "# len(embrf.episode_buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
